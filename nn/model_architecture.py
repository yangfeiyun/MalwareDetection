# This file is modified based on sample code from Joshua Saxe
from keras.models import Model
from keras import layers
from keras.layers import LeakyReLU


def my_model(input_length=2048):
    input = layers.Input(shape=(input_length,), dtype='float32', name='input')

    # Deep densely-connected network
    # x = layers.Dense(1024, activation='relu')(input)
    # x = layers.normalization.BatchNormalization()(x)
    # x = layers.Dense(512, activation='relu')(x)
    # x = layers.normalization.BatchNormalization()(x)
    # x = layers.Dense(64, activation='relu')(x)
    # x = layers.normalization.BatchNormalization()(x)

    # x = layers.Dense(1024)(input)
    # x = LeakyReLU(alpha=0.1)(x)
    # x = layers.normalization.BatchNormalization()(x)
    # x = layers.Dense(512)(x)
    # x = LeakyReLU(alpha=0.1)(x)
    # x = layers.normalization.BatchNormalization()(x)
    # x = layers.Dense(64)(x)
    # x = LeakyReLU(alpha=0.1)(x)
    # x = layers.normalization.BatchNormalization()(x)

    # x = layers.Dense(2048, activation='relu')(input)
    # x = layers.normalization.BatchNormalization()(x)
    # x = layers.Dense(1024, activation='relu')(x)
    # x = layers.normalization.BatchNormalization()(x)
    # x = layers.Dense(512, activation='relu')(x)
    # x = layers.normalization.BatchNormalization()(x)
    # x = layers.Dense(256, activation='relu')(x)
    # x = layers.normalization.BatchNormalization()(x)
    # x = layers.Dense(128, activation='relu')(x)
    # x = layers.normalization.BatchNormalization()(x)
    # x = layers.Dense(64, activation='relu')(x)
    # x = layers.normalization.BatchNormalization()(x)

    x = layers.Dense(2048)(input)
    x = LeakyReLU(alpha=0.1)(x)
    x = layers.normalization.BatchNormalization()(x)
    x = layers.Dense(1024)(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = layers.normalization.BatchNormalization()(x)
    x = layers.Dense(512)(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = layers.normalization.BatchNormalization()(x)
    x = layers.Dense(256)(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = layers.normalization.BatchNormalization()(x)
    x = layers.Dense(128)(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = layers.normalization.BatchNormalization()(x)
    x = layers.Dense(64)(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = layers.normalization.BatchNormalization()(x)

    # Finally add the last layer:
    output = layers.Dense(1, activation='sigmoid', name='output')(x)

    model = Model(inputs=input, outputs=output)
    model.compile(optimizer='sgd',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model


if __name__ == '__main__':
    model = my_model(2048)
